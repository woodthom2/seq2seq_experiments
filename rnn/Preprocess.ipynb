{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing script to prepare the biomedical training corpus for RNN training in TensorFlow\n",
    "\n",
    "\n",
    "Takes the original format text files (sentences file and annotations file) and converts them to a format that can be passed to the LSTM model.\n",
    "\n",
    "Input (in raw directory):\n",
    "\n",
    "   1. protein-train.ann - the annotations\n",
    "   2. protein-train.txt - the text\n",
    "\n",
    "Output (in preprocessed directory):\n",
    "\n",
    "   1. protein-train-ground-truth-annotations.txt - an IOB format file showing all proteins, where the tokens are aligned with the tokens from protein-train.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_dir = DATA_DIR + \"raw/\"\n",
    "preprocessed_dir = DATA_DIR + \"preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_annotations_file = raw_dir + 'protein-train.ann'\n",
    "raw_text_file = raw_dir + 'protein-train.txt'\n",
    "output_annotations_file = preprocessed_dir + '/protein-train-ground-truth-annotations.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tw_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "Protein = namedtuple('Protein', 'text within_doc_start_index within_doc_end_index sentence_no within_sentence_start_index within_sentence_end_index within_sentence_token_start_index within_sentence_token_end_index')\n",
    "Token = namedtuple('Token', 'text within_sentence_start_index within_sentence_end_index')\n",
    "Sentence = namedtuple('Sentence', 'start_index end_index text tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokeniser that we will use for protein data.\n",
    "\n",
    "Can't use normal NLP / NLTK tokeniser as it must be robust to strange punctuation that appears in biomed texts.\n",
    "\n",
    "Splitting on Unicode general category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokeniser = tw_utils.tw_protein_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokeniser(\"Smad1/5/8 Smad1?5?8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training sentences without annotations, and tokenise them\n",
    "\n",
    "This is quite ugly as the sentences will be tokenised a second time using the same method in the train procedure - in a production environment this would be done in a more unified way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    tokens = tokeniser(text)\n",
    "    current_index = 0\n",
    "    for t in tokens:\n",
    "        char_index = text.index(t, current_index)\n",
    "        current_index = char_index + 1\n",
    "        yield Token(t, char_index, char_index + len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_index = 0\n",
    "sentences = []\n",
    "sentence_to_proteins = []\n",
    "\n",
    "total_string = \"\"\n",
    "with open(raw_text_file, 'r') as f:\n",
    "    for l in f:\n",
    "        sentence_to_proteins.append([])\n",
    "        tokens = list(tokenise(l))\n",
    "        \n",
    "        sentence = Sentence(char_index, char_index + len(l), l, tokens)\n",
    "        sentences.append(sentence)\n",
    "        total_string += l\n",
    "        char_index += len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[1].start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign proteins to sentences\n",
    "\n",
    "Preprocess the list of annotations and raw text to produce a map from sentence to proteins and within-sentence char indices\n",
    "\n",
    "Preprocessing the input files is a little messy as the character indices in the annotations file don't align totally\n",
    "with the indices in the text file.\n",
    "Possibly this is due to my system (due to linebreaks, encoding, or using Python 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_proteins = []\n",
    "\n",
    "correction_to_add = 0\n",
    "sentence_id = 0\n",
    "tmpcorr = 0\n",
    "with open(raw_annotations_file, 'r') as f:\n",
    "    for idx, l in enumerate(f):\n",
    "            l = re.sub('\\s+$', '', l)\n",
    "            token_id, type_and_char_indices, correct_entity = l.split('\\t')\n",
    "            type_and_char_indices_split = type_and_char_indices.split(\" \")\n",
    "            start = int(type_and_char_indices_split[1])\n",
    "            end = int(type_and_char_indices_split[2])\n",
    "            extracted_entity_from_substring = total_string[(start + correction_to_add):(end + correction_to_add)]\n",
    "            # Sometimes the protein indices get out of alignment. In this case we search the nearby points\n",
    "            # in the document until we find where the protein has got to, normally it's only\n",
    "            # a few letters to the left or right.\n",
    "            if correct_entity != extracted_entity_from_substring:\n",
    "                for tmpcorr in [-1,1,-2,2,-3,3,-4,4,-5,5,-6,6,-7,7]:\n",
    "                    extracted_entity_from_substring = total_string[(start + correction_to_add + tmpcorr):(end + correction_to_add + tmpcorr)]\n",
    "                    if extracted_entity_from_substring == correct_entity:\n",
    "                        correction_to_add = correction_to_add + tmpcorr\n",
    "                        break\n",
    "            corrected_start = start + correction_to_add\n",
    "            corrected_end = end + correction_to_add\n",
    "            # Work out which sentence we're in\n",
    "            for tmp_sentence_id in range(sentence_id, len(sentences)):\n",
    "                if corrected_start >= sentences[tmp_sentence_id].start_index and corrected_start < sentences[tmp_sentence_id].end_index:\n",
    "                    sentence_id = tmp_sentence_id\n",
    "                    break\n",
    "            # Where does this protein start & end in the sentence in character indices?\n",
    "            protein_start_index_in_sentence = corrected_start - sentences[sentence_id].start_index\n",
    "            protein_end_index_in_sentence = corrected_end - sentences[sentence_id].start_index\n",
    "            \n",
    "            # Where are its begin and end tokens?\n",
    "            tokens = sentences[sentence_id].tokens\n",
    "            token_start_index, token_end_index = None, None\n",
    "            for token_idx, token in enumerate(tokens):\n",
    "                if token.within_sentence_start_index >= protein_start_index_in_sentence and token_start_index == None:\n",
    "                    token_start_index = token_idx\n",
    "                if token.within_sentence_end_index >= protein_end_index_in_sentence and token_end_index == None:\n",
    "                    token_end_index = token_idx\n",
    "            \n",
    "            assert token_start_index is not None and token_end_index is not None, \"TOKENS NOT ALIGNED\"\n",
    "                \n",
    "            if idx % 4000 == 0:\n",
    "                print correct_entity,\"tokens:\", tokens[token_start_index].text, tokens[token_end_index].text\n",
    "            \n",
    "            protein = Protein(correct_entity, corrected_start, corrected_end, sentence_id, protein_start_index_in_sentence, protein_end_index_in_sentence, token_start_index, token_end_index)\n",
    "            \n",
    "            \n",
    "            all_proteins.append(protein)\n",
    "            sentence_to_proteins[sentence_id].append(protein)\n",
    "            assert extracted_entity_from_substring == correct_entity, \"WARNING: ALIGNMENT FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[sentence_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = sentences[sentence_id].tokens\n",
    "token_start_index, token_end_index = None, None\n",
    "for token_idx, token in enumerate(tokens):\n",
    "    if token.within_sentence_start_index >= protein_start_index_in_sentence and token_start_index == None:\n",
    "        token_start_index = token_idx\n",
    "    if token.within_sentence_end_index >= protein_end_index_in_sentence and token_end_index == None:\n",
    "        token_end_index = token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protein_start_index_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protein_start_index_in_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the tokens to IOB notation\n",
    "\n",
    "This is really rough and ready, as the default tokenise method I'm using doesn't return character indices so I have to infer them to produce the correct annotation. It should work but it is an ugly way of coding so in a production environment this would be done properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bio_tokens(tokens, proteins):\n",
    "    bio_tokens = [\"O\"] * len(tokens)\n",
    "    for p in proteins:\n",
    "        bio_tokens[p.within_sentence_token_start_index] = \"BPROTEIN\"\n",
    "        for token_idx in range(p.within_sentence_token_start_index + 1, p.within_sentence_token_end_index + 1):\n",
    "            bio_tokens[token_idx] = \"IPROTEIN\"\n",
    "    return bio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(output_annotations_file, \"w\") as f:\n",
    "    for sentence_idx, sentence in enumerate(sentences):\n",
    "        proteins = sentence_to_proteins[sentence_idx]\n",
    "        bio_tokens = get_bio_tokens(sentence.tokens, proteins)\n",
    "        f.write(\" \".join(bio_tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\" \".join(bio_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
